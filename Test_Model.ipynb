{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9idusvNM78l",
        "outputId": "f7247849-f89f-4719-eaeb-c071570fd3ea"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers accelerate bitsandbytes unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGi5hTILMOJX",
        "outputId": "d072d00f-ae8a-4150-9198-3a4111c5b69d"
      },
      "outputs": [],
      "source": [
        "# Cargar modelo y tokenizador\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = \"AdrianML7/asesor-finanzas-phi3\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name, #Repo publico\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbU1cE63QM4P",
        "outputId": "f44e87c1-6952-4c9b-ab54-e7e620ac388b"
      },
      "outputs": [],
      "source": [
        "#Nuestro dataset tiene instruccion, input y output\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. The output must have a clean structure\n",
        "separated by paragraphs.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "instruction = input(\"Cual es tu pregunta? \")\n",
        "input_context = input(\"Cual es tu perfil? \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR1ryBcTQYqr"
      },
      "outputs": [],
      "source": [
        "#Preparar prompt\n",
        "\n",
        "#Adapta el prompt y tokeniza\n",
        "prompt = alpaca_prompt.format(instruction, input_context, \"\")\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "QDRwF0biReJu",
        "outputId": "eba27d50-1db8-4d71-857c-81c1bcffc6f6"
      },
      "outputs": [],
      "source": [
        "#Prepara la respuesta y muestrala por pantalla\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
        "texto_crudo = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "# Limpiamos para quedarnos solo con la respuesta del asistente\n",
        "respuesta_limpia = texto_crudo.split(\"### Response:\\n\")[-1].replace(\"<|endoftext|>\", \"\")\n",
        "\n",
        "# Renderizar Markdown\n",
        "display(Markdown(respuesta_limpia))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
