{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9idusvNM78l",
        "outputId": "f7247849-f89f-4719-eaeb-c071570fd3ea"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers accelerate bitsandbytes unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGi5hTILMOJX",
        "outputId": "d072d00f-ae8a-4150-9198-3a4111c5b69d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-602156311.py:3: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.11.6: Fast Mistral patching. Transformers: 4.57.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.11.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32064, 3072, padding_idx=32009)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=8192, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((3072,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cargar modelo y tokenizador\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = \"AdrianML7/asesor-finanzas-phi3\"\n",
        "#HF_TOKEN = \"TOKEN\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name, #Repo publico\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbU1cE63QM4P",
        "outputId": "f44e87c1-6952-4c9b-ab54-e7e620ac388b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cual es tu pregunta?Como ahorrar para la entrada de una vivienda?\n",
            "Cual es tu perfil?Joven estudiante en pr√°cticas, 800 euros de ahorro mensuales\n"
          ]
        }
      ],
      "source": [
        "#Nuestro dataset tiene instruccion, input y output\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. The output must have a clean structure\n",
        "separated by paragraphs.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "instruction = input(\"Cual es tu pregunta? \")\n",
        "input_context = input(\"Cual es tu perfil? \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XR1ryBcTQYqr"
      },
      "outputs": [],
      "source": [
        "#Preparar prompt\n",
        "\n",
        "#Adapta el prompt y tokeniza\n",
        "prompt = alpaca_prompt.format(instruction, input_context, \"\")\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "QDRwF0biReJu",
        "outputId": "eba27d50-1db8-4d71-857c-81c1bcffc6f6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Entiendo que, como estudiante, buscas construir un colch√≥n para tu futuro inmobiliario. Lo primero es definir un objetivo claro: por ejemplo, si deseas comprar en 5 a√±os, necesitar√°s acumular alrededor de 40‚ÄØ000‚ÄØ‚Ç¨ (asumiendo un precio de 80‚ÄØ000‚ÄØ‚Ç¨ y un enganche del 20‚ÄØ%). Con 800‚ÄØ‚Ç¨ al mes dispones de 9‚ÄØ600‚ÄØ‚Ç¨ al a√±o, lo que equivale a unos 720‚ÄØ‚Ç¨ mensuales despu√©s de impuestos (suponiendo una tasa del 20‚ÄØ%). \n",
              "\n",
              "**1. Fondo de emergencia**\n",
              "- Antes de invertir, destina entre 3 y 6 meses de gastos a una cuenta de alta liquidez (cuenta de ahorros o dep√≥sito a corto plazo). Esto te protege ante imprevistos y evita que tengas que vender inversiones en momentos desfavorables.\n",
              "\n",
              "**2. Ahorro sistem√°tico**\n",
              "- Programa una domiciliaci√≥n autom√°tica de 720‚ÄØ‚Ç¨ al mes a una cuenta de ahorro o a un plan de pensiones con ventajas fiscales. La automatizaci√≥n reduce la tentaci√≥n de gastar ese dinero.\n",
              "\n",
              "**3. Inversi√≥n de bajo riesgo**\n",
              "- Con un horizonte de 5 a√±os y un perfil conservador, puedes destinar una parte (por ejemplo, 50‚ÄØ% del ahorro mensual) a instrumentos de renta fija de corto plazo, como bonos del Estado o fondos de bonos de alta calidad. Estos ofrecen rendimientos modestos pero estables y bajos volatilidad.\n",
              "- El resto (50‚ÄØ%) puede ir a un fondo indexado de acciones de gran capitalizaci√≥n (ej. MSCI World) con exposici√≥n global, que hist√≥ricamente ha superado la inflaci√≥n a medio‚Äëlargo plazo. Aunque la volatilidad es mayor, el horizonte de 5 a√±os permite suavizar los altibajos.\n",
              "\n",
              "**4. Revisi√≥n y ajuste**\n",
              "- Cada 12‚ÄØmeses revisa tu progreso: si tu ahorro supera el objetivo, puedes aumentar el enganche o destinar m√°s a inversiones de mayor potencial de crecimiento.\n",
              "- Si en alg√∫n momento tu situaci√≥n cambia (p.‚ÄØej., ingresos mayores o gastos inesperados), ajusta el porcentaje destinado a ahorro y a inversi√≥n.\n",
              "\n",
              "**Advertencia de riesgo**\n",
              "- Todas las inversiones conllevan riesgo de p√©rdida de capital. Los fondos de renta fija pueden verse afectados por cambios en las tasas de inter√©s, y los fondos indexados de acciones pueden experimentar ca√≠das de valor en el corto plazo. Aseg√∫rate de que el dinero que no necesites en los pr√≥ximos 5 a√±os no est√© en instrumentos de alta volatilidad.\n",
              "\n",
              "Con disciplina y una estrategia bien estructurada, podr√°s acumular el enganche necesario para tu vivienda sin comprometer tu estabilidad financiera actual."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Prepara la respuesta y muestrala por pantalla\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
        "texto_crudo = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "# Limpiamos para quedarnos solo con la respuesta del asistente\n",
        "respuesta_limpia = texto_crudo.split(\"### Response:\\n\")[-1].replace(\"<|endoftext|>\", \"\")\n",
        "\n",
        "# Renderizar Markdown\n",
        "display(Markdown(respuesta_limpia))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
